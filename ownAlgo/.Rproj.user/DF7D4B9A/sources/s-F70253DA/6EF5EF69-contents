# neural networks


library(keras)
library(tibbletime)
library(dplyr)
data2 <- fin
data2<- readRDS('nnData.rds')
# reshape
seconds_in_day <-  24*60*60

# prepare NL data for ML
d <- data2 %>%
  mutate( day  = strftime(time,"%u"),
          month = strftime(time,"%m"),
          secMidnight = lubridate::period_to_seconds(lubridate::hms(strftime(time,"%T"))),
          sinTime = sin(2*pi*secMidnight/seconds_in_day),
          cosTime = cos(2*pi*secMidnight/seconds_in_day)) %>% 
  select(time,clust,nextDist,prevDist,nextMeas,prevMeas,day,sinTime,cosTime,month) %>% 
  mutate(prevDist = scales::rescale(prevDist),
         nextDist = scales::rescale(nextDist),
         nextMeas = scales::rescale(nextMeas),
         prevMeas = scales::rescale(prevMeas),
         lagClust = lag(clust),
         leadClust = lead(clust))

d <- d %>% na.omit()# remove first and previous lag difference

# If random sample
## 75% of the sample size
smp_size <- floor(0.75 * nrow(d))
train_ind <- sample(seq_len(nrow(d)), size = smp_size)

train <- d[train_ind,]
test <- d[-train_ind,]
#atlernative

train <- d %>% as_tbl_time(time)%>%
  filter_time(("start"~ "2017-03-12"))

train <- rbind(train, d %>% as_tbl_time(time)%>%
                 filter_time("2017-03-14" ~ "end"))

test <- d %>% as_tbl_time(time)%>%
  filter_time((~ "2017-03-13"))

# alternative end
nClust<- (length(unique(allPoints$clust))+1)

x_train <- cbind(train[,c(5,6,8,9)],
                 to_categorical(train$day, num_classes = 8),
                 to_categorical(train$month, num_classes = 13),
                 to_categorical(train$lagClust,num_classes = nClust),
                 to_categorical(train$leadClust, num_classes = nClust)) %>% as.matrix()

x_test <- cbind(test[,c(5,6,8,9)],
                 to_categorical(test$day, num_classes = 8),
                 to_categorical(test$month, num_classes = 13),
                 to_categorical(test$lagClust,num_classes = nClust),
                 to_categorical(test$leadClust, num_classes = nClust)) %>% as.matrix()


y_train <- to_categorical(train$clust, num_classes = nClust)
y_test <- to_categorical(test$clust, num_classes = nClust)


model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 200, activation = 'relu', input_shape = ncol(x_test)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 120, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = nClust, activation = 'softmax')

summary(model)

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

history <- model %>% fit(
  as.matrix(x_train), as.matrix(y_train), 
  epochs = 10, batch_size = 200, 
  validation_split = 0.2
)
#plot(history)

model %>% evaluate(as.matrix(x_test),as.matrix(y_test))

# show uncertainty as a function of distance in time from measurements. 
model %>% predict_proba(as.matrix(x_test))

t<- test %>% mutate(
  predC = as.integer(model %>% predict_classes(as.matrix(x_test)))
) %>% as_tbl_time(time) %>% 
  filter_time(~"2017-03-13")

library(leaflet)

leaflet() %>% 
  addTiles() %>% 
  addCircles(data = left_join(t,allPoints, by = "clust"),lng = ~lon, lat = ~lat, color = "red", radius = 10,
             label = ~as.character(clust)) %>% 
  addCircles(data = left_join(t,allPoints, by = c("predC" = "clust")),lng = ~lon, lat = ~lat, color = "green", radius = 10,label = ~as.character(predC))

t %>% select(time, clust,predC) %>% View()

# identify & extract missings

d %>% filter( nextMeas > 60 )
