---
title: 'Handling missing data in smartphone location logs'
author: "Boaz Sobrado"
date: "`r format(Sys.time(), '%d %B %Y')`"
bibliography:
  - thesisGPS.bib
  - rReferences.bib
output:
  rmarkdown::pdf_document:
    citation_package: natbib
    fig_caption: yes        
    includes:  
      in_header: tp2.tex

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(ggthemes)
library(ggplot2)
library(knitr)
```

# Abstract
*Using objective location data to infer the mobility measures of individuals is highly desirable, but methodologically difficult. Using commercially gathered location logs from smartphones holds great promise, as they have already been gathered, often span years and can be associated to individuals. However, due to technical constraints this data is more sparse and inaccurate than that produced by specialised equipment. In this paper we present a model which leverages the periodicity of human mobility in order to impute missing data values. Moreover, we will assess the performance of the model relative to currently used methods, such as linear interpolation.*

# Introduction

How active people are and how they interact with their environment affects a wide range of outcomes including health, income and social capital [@goodchild_toward_2010]. A better understanding of both within-person and between-person variability in geospatial patterns could be conducive to better social, health and urban-planning policies. Yet a large part of studies on human mobility are largely based on pen-and-paper travel diaries. These surveys have known methodological flaws, such as the short period of data collection (due to costs and burden to respondents), the underreporting of short trips [@wolf_impact_2003] and the underestimation of the duration of commutes [@delclos-alio_keeping_2017]. 

These obstacles can be overcome by using objective data on human mobility. The Global Positioning System (GPS), which uses the distance between a device and a number of satellites to determine location, provides such data. Within behavioural science, this type of data has been used to investigate topics such as the effects of the food environment on eating patterns [@zenk_how_2009], the movement correlates of personality [@harari_using_2016], academic performance [@wang_smartgpa:_2015] and bipolar disorder [@palmius_detecting_2017].

In most of these studies participants are given a specialised device, resulting in accurate mobility GPS data (*specialised logs*). However, @barnett_inferring_2016 point out that these studies are not scalable due to cost and burden to participants. Moreover, they may be biased because of the introduction of a new device to the participant's life. Because of these drawbacks, specialised logs usually span a short amount of time. @barnett_inferring_2016 advocate installing a custom-made tracking app on user's phones (*custom logs*). Another solution is to take advantage of existing smartphone location logs, such as Google Location History, which store location information of millions of users spanning several years [@location_history_timeline_nodate] (*secondary logs*). By law, logs can be accessed and shared by users for free [*European Comission Reference*]. Yet, because they were created for non-academic purposes under engineering constraints, the sensors do not monitor continuously and the resulting logs can be sparse and inaccurate. Hence, two important challenges are dealing with measurement noise and missing data (*messy data*).

Missing data is a pervasive issue as it can arise due to multiple reasons. Technical reasons include signal loss, battery failure and device failure. Behavioural reasons include leaving the phone at home, switching the phone off, switching location measurements off, and so on. As a result, applied researchers are often left with wide temporal gaps with no measurements. For instance, different groups studying the effect of bipolar disorder on human movement have reported missing data rates between 30% to 50% [@saeb_mobile_2015;@grunerbl_smartphone-based_2015;@palmius_detecting_2017]. Similar trends are consistently reported in other fields [e.g. @harari_using_2016;@jankowska_framework_2015].

There is currently no golden standard in how to deal with missing data in custom or secondary logs [@barnett_inferring_2016]. Traditional missing data methods, such as mean imputation, cannot be used easily in spatiotemporal data because the measurements are correlated in time and space. For example, assume the simplistic case that an individual spends almost half her time at work, half her time at home and a small amount of time commuting between the two along a angled path. Using mean imputation would result in imputed values of her being at the geometric midpoint between the home and work for all missing values, even though she has never been there and never will be. Worringly, @jankowska_framework_2015 have pointed out that there is often little transparency regarding decisions of how to deal with missing data.

The accuracy of GPS measurements in smartphones is substantially lower than in professional grade GPS trackers. For example, Android phones collect location information through a variety of methods, such as from WiFi access points, cellphone triangulation, and GPS measurements. They use different methods due to computational and battery constraints [@lamarca_place_2005; @chen_practical_2006].In professional grade GPS trackers less than 80% of measurements fall within 10 meters of the true location. GPS measures are reported to be most inaccurate in high density urban locations and indoors [@schipperijn_dynamic_2014;@duncan_portable_2013]. Unfortunately for social scientists, this happens to be where most people in the developed world tend to spend most of their time.

On the other hand, noisy data can lead to inaccurate conclusions if it is not accounted for, such as overestimating the movement of individuals. For instance, suppose that a naive researcher calculates the distance travelled by an individual by drawing a line between each measured point and calculating the sum of the length of all of these lines. If there is noise, the measurements will vary even though the individual is not moving. If the measurements are frequent, then the researcher will end up with a lot of movement, even though the individual did not move at all. The problem is further complicated by the fact that missing data and noisy measurements are related to each other. For instance, methods used by researchers to reduce noise, such as throwing out inaccurate measurements [e.g. @palmius_detecting_2017] can exacerbate the severity of the missing data problem.

In this paper we will compare methods used to deal with measurement error and missing data in mobility patterns from secondary GPS logs.

# A concrete example

Given that there is little literature on dealing with missing data in custom or secondary logs it is worth illustrating the typical characteristics of this data using an example data set. The example dataset comes from the Google Location History of a single individual and spans from January 2013 to January 2017. It was recorded with multiple different Android devices and contains 814 941 measurements, with approximately 742 measurements per day ($\widehat{\sigma}$=868.15). The dataset contains a wide range of variables including inferred activity and velocity. For the purposes of this paper we will focus only on latitude, longitude, accuracy (defined below) and a timestamp.

## Location logs and notation

GPS measurements report our location on a three dimensional planet, yet we are interested in placing these measurements on a two dimensional map. Projecting three dimensional measurements onto a two dimensional plane results in errors, in order to minimise these errors we borrow an error minimising projection method from @barnett_inferring_2016. 

Let a persons' true location on this two-dimensional plane be $G(t) = [G_x(t) G_y(t)]$ where $G_x(t)$ and $G_y(t)$ denote the location of the individual at time $t$ on the x-axis and y-axis respectively. Moreover, let $D \in \mathbb{R}^2$ be the recorded data containing lattitude and longitude. In addition, let $a$ denote the estimated accuracy of the recorded data.  accuracy. $G(t)$, $D$ and $a$ are indexed by time labled by the set $T = t_1 < ... < t_{n+1}$. For simplicity, let each entry in the discrete index set $T$ represent a 5 minute window. The measure of accuracy $a_t$ is given in meters such that it represents the radius of a 67% confidence circle. If $D_t = \emptyset$ it is considered *missing* and it is not missing otherwise. 

### Accuracy in location logs 

In the example data set the distribution of $a$ is highly right skewed, with a median of 28, $\mu = 127$ and the maximum value at 26 km. @palmius_detecting_2017 note that in their Android based custom logs inaccurate location values are interspersed between more accurate location values at higher sample rates per hour. We observe similar patterns in secondary logs. Figure 1 shows how accuracy tends to vary as a function of user behaviour, time and location. There are several recurring low-accuracy points, most likely cell-phone triangulation towers.  

```{r, accuracyPlot,echo = FALSE,fig.cap="Measurement accuracy of each logged measurement in a morning journey. The red circles denote the accuracy of all logged measurement points (the raw data). The points connected in time are connected by a line. The blue line shows the path without the most inaccurate (accuracy > 400 meters) points filtered out. The red line shows the path with all measurements included. ",out.width = '100%'}
include_graphics("img/journeyTillMiddayBoaz.png")
```


```{r, accuracyPlot2, echo=FALSE, fig.cap="The accuracy of raw measures with time on the x-axis and accuracy on the y-axis. The colour scale shows the distance between the measurement and the previous point.",out.width = '100%'}
include_graphics("img/accuracyLocShift.png")

```

### Missingness

Over 54% of the data is missing for the entire duration of the log. However,this is misleading as there are several long periods with no measurements whatsoever. The structure of missingness of a day with measurements is shown below. 

```{r, measurementsPerDay,echo = FALSE,fig.cap="Example of missing data over a day. The x-axis denotes time, the y-axis shows how many measurements are made and each point is a five minute window. For this day there were several periods with no information. These points are filled with red and lie on the x-axis.",out.width = '100%'}
#fix width
include_graphics("img/missingBoaz5minExample.png")

```

## Current methods for imputing & filtering spatiotemporal data

As we have mentioned before, @jankowska_framework_2015 point out that there is little transparency regarding decisions of how to deal with missing data in GPS mobility logs. We suspect this is because the highly interdisciplinary nature of the problem means researchers are unaware of potential solutions. For this reason, we consider it important to briefly discuss a few methods one could consider in order to deal with measurement inaccuracy and missing data problems in spatiotemporal data. In doing so, we will argue that state space models (SSMs) and spatiotemporal imputation methods, which are extensively used to model missing and noisy spatiotemporal data, are not well suited to deal with messy human mobility patterns. Moreover, we discuss in detail two approaches by @palmius_detecting_2017 and @barnett_inferring_2016 which deal explicitly with missing data in mobility patterns from smartphone GPS logs.

### General Spatiotemporal Methods

There is a vast literature of using SSMs to improve measurements accuracy and deal with missing data. Behavioural ecologists for instance, have used SSMs extensively to explain how animals interact with their environment [@patterson_statespace_2008]. These models can be quite complex, for example @preisler_modeling_2004 uses Markovian movement processes to characterise the effect of roads, food patches and streams on cyclical elk movements. The most well studied SSM is the Kalman filter, which is the optimal algorithm for inferring linear Gaussian systems. In fact, the extended Kalman filter is the de facto standard for GPS navigation [@chen_state_2013]. The advantage of state space models is that they are flexible, deal with measurement inaccuracy, include information from different sources and can be used in real time.

However, the main limitation of SSMs is that they ignore the fact that humans have regular movement routines, such as going to work or shopping for groceries on weekends. This limitation is due to the fact that SSMs are based on the Markov property. Thus, the estimated location $G(t)$ at timepoint $t$ is often based only upon measurements $D_t$, $D_{t-1}$ and ignores all $D_{t-i}|i\geq2$. Hierarchical structuring and conditioning on a larger context have been suggested as ways to improve the performance of Markovian models, but these solutions are often computationally intractable or unfeasible [@sadilek_far_2016]. For this reason we do not consider SSMs to be useful for imputing missing data, although they could be of use in filtering noise.

Spatiotemporal imputation methods generally assume  fixed measurement stations there are several imputation methods for spatiotemporal measurements. For instance, @feng_cutoff:_2014 illustrate their CUTOFF method, which relies on estimating missing values using the nearest observed neighbours in time, using rainfall data from dozens of gauging stations across Australia. Similarly, @zhang_application_2017 use a variety of machine learning methods to present their model based on underground water data in China.

While @feng_cutoff:_2014 claim their model could be used to establish mobility patterns, ostensibly by dividing the sample space into rasters analogous to measurement stations indicating a probability of the individual being there, this seems to be computationally unfeasible. To our knowledge such models have not been implemented.  


### Filtering & Mean imputation

@palmius_detecting_2017 deal with measuremement inaccuracy of $D$ by removing from the data set all unique low-accuracy $a$ data points that had  $\frac{d}{dt}D > 100 \frac{km}{h}$. Subsequently the researchers down sample the data to a sample rate of 12 per hour using a median filter. Moreover, @palmius_detecting_2017 explain:

>"If the standard deviation of [$D$] in both latitude and longitude within a 1 h epoch was less than 0.01 km, then all samples within the hour were set to the mean value of the recorded data, otherwise a 5 min median filter window was applied to the recorded latitude and longitude in the epoch".

Missing data was imputed using the mean of measurements close in time if the participant was recorded within 500m of either end of a missing section and the missing section had a length of $\leq 2h$ or $\leq 12h$ after 9pm. 

### Barnett's model

@barnett_inferring_2016 deal with custom logs where location is measured for 2 minutes and subsequently not measured for 10 minutes. @barnett_inferring_2016 handle missing data by:

> "simulat[ing] flights and pauses over the period of missingness where the direction,
duration, and spatial length of each flight, the fraction of flights versus the fraction of
pauses, and the duration of pauses are sampled from observed data."

This method can be extended to imputing the data based on temporally, spatially or periodically close flights and pauses. In other words, for a given missing period, the individual's mobility can be estimated based on  measured movements in that area, at that point in time or movements in the last 24 hours.

This paper is to the best of our knowledge the only attempt at establishing a principled approach to this problem.

*perhaps remove the section on alternative for the december deadline*
### Alternative models

Alternatives to state space models include long range-persistence models, such as cascading walks models  and the FarOut model which rely on self-similarity and autoregressive characteristics [@han_cascading_2015; @sadilek_far_2016]. The latter uses Fourier analysis and PCA to extract cyclical patterns in an individual's behaviour and reduce the dimensionality of the extracted features and yields interpretable predictions for an individuals location months in advance. 


## Methods

### Datasets & Analyses

The data used to train the imputation methods was collected between 2013 and 2017 on different Android devices from several individuals. The table below provides more details:

```{r datadetailsTable, echo = F}
datadetails <- readRDS("tempdata/datadescriptives.rds")
datadetails <- datadetails[,-7] #remove last column for now
kable(datadetails,digits = 2,col.names = c("Log duration",
                                           "Logged days",
                                           "Observations",
                                           "Missing days",
                                           "Missing data",
                                           "Mean Accuracy"),
      caption = "Table with descriptives about the data sets used to build the imputation methods. ")
```

In addition to the secondary logs, participants also volunteered to carry with them a specialised GPS tracker for a week. This specialised log was used to evaluate the models. 

Analyses were performed using R and a multitude of other packages [@ggplot2;@dplyr;@leaflet; @ggthemes;@base;@sp1;@sp2].

### Data pre-processing

Three different filtering methods were tested. First, the filtered rolling-median downsampling method described by @palmius_detecting_2017. The second method is a weighted mean approach taking $f(a)$ as a weight. Finally, an extended Kalman filter. The output of all of these methods was taken as the input of the imputation methods.  

### Imputation methods

Four imputation methods were selected in order to cover techniques applied in the literature. Briefly, the mean imputation method described by @palmius_detecting_2017, the model developed by @barnett_inferring_2016 as well as an adapted FarOut model [@sadilek_far_2016]. Simple linear interpolation was used as a benchmark model. 

### Evaluation criteria

The entire length of the secondary logs were used as a training set. The specialised logs were used as a test set. The missing data imputation models were evaluated both directly, and on on two computed measures: amount of trips made and distance traveled.

The direct evaluation involved calculating the error of each $D_t$ compared to $G(t)$ approximated by the specialised log. The error measures used were root mean square error (RMSE) and mean absolute error (MAE).

The evaluation on computed measures involved calculating a mobility trace following the rectangual method of @rhee_human_2007 for each imputed dataset. Like @barnett_inferring_2016 we calculate bias by substracting the estimated measure under each approach for the same measure calculated on the full data. For simulation-based imputation approaches a mean value over 100 samples was taken.

## Results

## Discussion


## References
